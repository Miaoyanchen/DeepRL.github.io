%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for CMY at 2024-05-10 15:05:40 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@electronic{rmsprop,
	author = {Vitaly Bushaev},
	date-added = {2024-05-10 15:03:41 -0400},
	date-modified = {2024-05-10 15:04:41 -0400},
	month = {September},
	title = {Understanding RMSprop --- faster neural network learning},
	url = {https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a},
	year = {2018}}

@url{youtube,
	address = {https://youtu.be/TmPfTpjtdgg},
	author = {Google Deepmind},
	date = {2016},
	date-modified = {2024-05-09 01:54:47 -0400},
	month = {June},
	organization = {Youtube},
	title = {DQN Breakout}}

@online{DeepRL,
	address = {https://youtu.be/teDuLk3cIeI},
	author = {LLMs Explained-Aggregate Intellect-AI.SCIENCE},
	date = {2018},
	month = {December},
	organization = {Youtube},
	title = {Deep Q-Learning paper explained: Human-level control through deep reinforcement learning (algorithm)}}

@article{Kaelbling1998,
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.},
	author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
	date-added = {2024-05-07 18:27:40 -0400},
	date-modified = {2024-05-07 18:28:00 -0400},
	doi = {https://doi.org/10.1016/S0004-3702(98)00023-X},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Planning, Uncertainty, Partially observable Markov decision processes},
	number = {1},
	pages = {99-134},
	title = {Planning and acting in partially observable stochastic domains},
	url = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
	volume = {101},
	year = {1998},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
	bdsk-url-2 = {https://doi.org/10.1016/S0004-3702(98)00023-X}}

@article{Mnih2015,
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	date = {2015/02/01},
	date-added = {2024-05-07 12:08:32 -0400},
	date-modified = {2024-05-07 12:08:41 -0400},
	doi = {10.1038/nature14236},
	id = {Mnih2015},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7540},
	pages = {529--533},
	title = {Human-level control through deep reinforcement learning},
	url = {https://doi.org/10.1038/nature14236},
	volume = {518},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1038/nature14236}}

@book{sutton2018,
	author = {Sutton, R.S. and Barto, A.G.},
	date-added = {2024-05-07 11:28:11 -0400},
	date-modified = {2024-05-07 11:28:31 -0400},
	isbn = {9780262039246},
	lccn = {2018023826},
	publisher = {MIT Press},
	series = {Adaptive Computation and Machine Learning series},
	title = {Reinforcement Learning, second edition: An Introduction},
	url = {https://books.google.com/books?id=5s-MEAAAQBAJ},
	year = {2018},
	bdsk-url-1 = {https://books.google.com/books?id=5s-MEAAAQBAJ}}
